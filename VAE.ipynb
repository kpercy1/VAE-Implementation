{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cR2wBH7bGZhK"
   },
   "source": [
    "# VAE Theory\n",
    "\n",
    "All theory is from https://arxiv.org/abs/1606.05908.\n",
    "\n",
    "Notation:\n",
    "- $X$ is a data point in our dataset\n",
    "- $z$ is the latent representation of a data point\n",
    "- $\\theta$ is the parameters for our model, which will be a distribution for $P(X|z;\\theta)$. Specifically, our model is a mapping from the latent representation to a data point.\n",
    "- $D$ is our entire dataset, so $X\\in D$.\n",
    "<!-- \n",
    "- $f(z;\\theta)$ is a map from latent variables z to a point estimate for a data point, X.)\n",
    "<> - $Q(z|X)$ is a \"surrogate\" distribution to approximate the probability $P(z|X)$. Assuming we choose that Q(z|X) is normal, then $\\mu(X;\\phi)$ and $\\Sigma(X;\\phi)$ are the mean and variance, respectively.\n",
    "-->\n",
    "\n",
    "\n",
    "Explanation\n",
    "- We wish to maximising the probability of our data under our model,\n",
    "\\begin{equation}\n",
    "P(X) = \\int P(X|z; \\theta) P(z)dz.\n",
    "\\end{equation}\n",
    "- When $X$ is continuous, it is common to use a normal distribution for our model:\n",
    "$$P(X|z; \\theta) = N(f(z;\\theta), \\sigma^2 I).$$\n",
    "Here, $f(z;\\theta)$ is our point estimate for a mapping from the latent representation to the data points, i.e. $f:\\mathcal{Z} \\times \\Theta\\rightarrow \\mathcal{X}$, where any $z\\in \\mathcal{Z}$, $\\theta \\in \\Theta$ and $X \\in \\mathcal{X}$. Also, $\\sigma^2$ is a hyperparameter of the model, and is set beforehand.\n",
    "- We say that the samples of $z$ are from a $N(0, I)$ distribution, so $f(z; \\theta)$ is left with the responsiblity of first converting these normal samples into latent variables which are more representative/useful. This uses the fact that any distribution in $d$ dimensions can be generated by taking a set of $d$ variables that are normally distributed and mapping them through a sufficiently complicated function (this is an extension of inverse transform sampling).\n",
    "- To allow sufficient flexibility, $f(z;\\theta)$ is usually a multi-layer neural network. This allows us to learn a map from $z\\sim N(0, \\sigma^2 I)$ to something which \"looks like\" a data point, $X$. \n",
    "- A naive approach would be to sample $z_1, ..., z_n$ for very large $n$, then approximate our model evidence as $P(X) \\approx \\frac{1}{n} \\sum_{i=1}^n P(X|z_i; \\theta)$. From here we could use gradient descent to optimise $f(z; \\theta)$ by maximising $P(X)$. The problem is that in higher dimensional space, $n$ might have to be extremely large before we get an accurate estimate. This is because usually, for most $z$, $P(X|z)$ will be nearly zero. The key idea of Variational Autoencoders is to attempt to sample values of $z$ that are likely to have produced $X$, and then compute $P(X)$ just from these...\n",
    "- The definition of KL-divergence is\n",
    "\\begin{equation}\n",
    "D_{KL}(P||Q) = \\int P(x) \\log \\left(\\frac{P(x)}{Q(x)}\\right)dx.\n",
    "\\end{equation}\n",
    "Equivalently,\n",
    "\\begin{equation}\n",
    "D_{KL}(P||Q) = \\mathbb{E}_{X\\sim P} \\left[ \\log\\left(\\frac{P(x)}{Q(x)}\\right)\\right] = \\mathbb{E}_{X\\sim P} \\left[ \\log P(x) - \\log Q(x) \\right].\n",
    "\\end{equation}\n",
    "- In VAEs, we explained earlier that we want to sample the values of $z$ which are likely to have produced any particular data point, $X$. We use a \"surrogate\" distribution to approximate $P(z|X)$, $Q(z|X)$. Then, the KL-divergence between our surrogate distribution $Q(z|X)$ and the probability of latent variables in a particular data point, $P(z|X)$ is given by\n",
    "\\begin{equation*}\n",
    "D_{KL}(Q(z|X) || P(z|X)) = \\mathbb{E}_{z\\sim Q} \\left[ \\log Q(z|X) - \\log P(z|X) \\right].\n",
    "\\end{equation*}\n",
    "Now we can apply Bayes Rule:\n",
    "\\begin{align}\n",
    "P(z|X) &= \\frac{P(X|z) P(z)}{P(X)} \\implies D_{KL}(Q(z|X) || P(z|X))\\\\ \n",
    "&= \\mathbb{E}_{z\\sim Q} \\left[ \\log Q(z|X) - \\log \\left( \\frac{P(X|z) P(z)}{P(X)}\\right)\\right]\\\\\n",
    "&= \\mathbb{E}_{z\\sim Q} \\left[ \\log \\left( \\frac{Q(z|X)}{P(z)}\\right) - \\log P(X|z) \\right] + \\log P(X)\\\\\n",
    "&= D_{KL}(Q(z|X) || P(z)) - \\mathbb{E}_{z\\sim Q}\\left[ \\log P(X|z) \\right] + \\log P(X).\\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\therefore \\log P(X) - F_{KL}(Q(z|X) || P(z)) = \\mathbb{E}_{z\\sim Q}\\left[ \\log P(X|z)\\right] - D_{KL}(Q(z|X) || P(z))\n",
    "\\end{equation*}\n",
    "\n",
    "We cannot compute $D_{KL}(Q(z|X) || P(z|X))$ without knowing $P(z|X)$, but a property of KL-divergence is that it is non-negative. Therefore, our expression becomes\n",
    "\\begin{equation*}\n",
    "\\log P(X) \\geq \\mathbb{E}_{z\\sim Q}\\left[ \\log P(X|z)\\right] - D_{KL}(Q(z|X) || P(z)).\n",
    "\\end{equation*}\n",
    "The LHS is called the evidence, hence the RHS is called the \"Evidence Lower BOund\" (ELBO). We wish to maximies the evidence (as this corresponds to the observed data being likely under our model), so in VAEs we do this by maximising the RHS and treating $D_{KL}(Q(z|X) || P(z|X))$ as an error term.\n",
    "- Typically, we set our surrogate distribution as $Q(z|X) \\sim N(\\mu(X; \\phi), \\Sigma(X; \\phi))$. In practice, $\\mu(X; \\phi)$ and $\\Sigma(X;\\phi)$ are again obtained using neural networks, and these are maps from a data point $X$ to the estimated mean and variance for the latent variables, respectively. Hence, the parameters $\\phi$ are learned from the data. Note that $\\Sigma$ is constrained to be a diagonal matrix, as independence between components is necessary for computational tractibility and encourages disentanglement of latent variables.\n",
    "- In the ELBO, the term $D_{KL}(Q(z|X) || P(z))$ is a KL-divergence between multivariate Gaussian distributions, hence a closed form expression for the KL-divergence is known. In our case, \n",
    "\\begin{align}\n",
    "D_{KL}(Q(z|X) || P(z)) &= D_{KL}(N(\\mu(X), \\Sigma(X)) || N(0, I))\\\\\n",
    "&= \\frac{1}{2}\\left( \\text{trace}(\\Sigma(X)) + (\\mu(X))^T (\\mu(X)) - k - \\log \\text{det}(\\Sigma(X))\\right)\n",
    "\\end{align}\n",
    "\n",
    "But, in the ELBO it is more difficult to compute the term $\\mathbb{E}_{z \\sim Q} \\left[ log P(X|z) \\right]$. One approach is to estimate this expectation by sampling many $z$, although this will be expensive. Instead, we can apply the idea from stochastic gradient descent and use a single sample of $z$ to approximate this expectation for a single data point, X. Then, our total loss (over the entire dataset $D$) to minimise with stochastic gradient descent is\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{X\\sim D} \\left[ \\mathbb{E}_{z \\sim Q} \\left[ \\log P(X|z) \\right] - D_{KL} (Q(z|X) || P(z)) \\right].\n",
    "\\end{equation}\n",
    "We estimate the gradient of this by averaging the gradient of\n",
    "\\begin{equation}\n",
    "\\log P(X|z) - D_{KL}(Q(z|X) || P(z))\n",
    "\\end{equation}\n",
    "for an arbitrary number of samples of $z$ and $X$. The result will converge to the overall gradient (for all $X$, $z$).\n",
    "\n",
    "- But, the issue with the above gradient estimate is that we can't actually backpropogate through $\\mathbb{E}_{z\\sim Q} \\left[ \\log P(X|z) \\right]$, as the expectation $\\mathbb{E}_{z \\sim Q} \\left[ \\cdot \\right]$ depends on $\\phi$. Specifically, in general $\\triangledown_{\\phi} \\mathbb{E}_{z\\sim Q} \\left[ \\log P(X|z) \\right] \\neq \\mathbb{E}_{z\\sim Q} \\left[ \\triangledown_{\\phi} \\log P(X|z) \\right]$. In integral form,\n",
    "\\begin{align}\n",
    "\\triangledown_{\\phi} \\mathbb{E}_{z\\sim Q} \\left[ \\log P(X|z) \\right] &= \\int_z \\triangledown_{\\phi} ( Q(z|X) \\log P(X|z) ) dz\\\\\n",
    "&= \\int_z \\triangledown_{\\phi} (Q(z|X)) \\log P(X|z)dz + \\int_z Q(z|X) \\triangledown_{\\phi} ( \\log P(X|z) ) dz\\\\\n",
    "&= \\int_z \\triangledown_{\\phi} (Q(z|X)) \\log P(X|z)dz + \\mathbb{E}_{z \\sim Q} \\left[ \\triangledown_{\\phi} (\\log P(X|z) ) \\right].\n",
    "\\end{align}\n",
    "\n",
    "We get around this issue using the reparametrisation trick. We instead convert the random sampling into a (stochastic) input, and then write $Q(z|X)$ as a deterministic function of these stochastic inputs. For example, when $Q(z|X)\\sim N(\\mu(X; \\phi), \\Sigma(X; \\phi))$ can be written as \n",
    "$$Q(z|X) = \\mu(X; \\phi) + (\\Sigma(X; \\phi))^{\\frac{1}{2}} \\cdot \\epsilon,$$ \n",
    "where $\\epsilon \\sim N(0, I)$. In general, we want to sample from $Q(z|X)$ by evaluating a function $h(\\nu, X)$, where $\\nu$ is noise from a distribution that is not learned (i.e. has fixed inputs that don't depend on $\\phi$). Also, $h$ must be continuous in $X$ so that we can backpropogate through it. \n",
    "\n",
    "Things to estimate in VAE:\n",
    "- $f(z; \\theta)$, neural network. Maps $z$ to $X$.\n",
    "- $\\mu(X; \\phi)$ and $\\Sigma(X; \\phi)$, neural network. Parameters of the surrogate $Q(z|X)$ for $P(z|X)$. $\\mu$ maps the data point $X$ to a point estimate of the mean for the latent variables $z$, and $\\Sigma$ maps $X$ to the estimated variance for $z$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\kperc\\vae\\.venv\\lib\\site-packages (6.1.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\kperc\\vae\\.venv\\lib\\site-packages (from plotly) (1.40.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kperc\\vae\\.venv\\lib\\site-packages (from plotly) (25.0)\n"
     ]
    }
   ],
   "source": [
    "#FIX THIS\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first apply to the MNIST Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "W0612 09:26:35.155000 17528 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#Code for training speed-up\n",
    "#https://www.youtube.com/watch?v=VwQEND_aTfU\n",
    "import torch\n",
    "import torchao\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "#Include once model is initialised:\n",
    "#model = torchao.autoquant(torch.compile(model))\n",
    "#Include when model is being run:\n",
    "#with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "#   #Run model \n",
    "\n",
    "#Check for GPU\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GcKNKGJFGSO1"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "# Transform: Normalize and flatten\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), #Shape: (1, 28, 28)\n",
    "    transforms.Normalize((0.5,), (0.5,)), # normalize to [-1, 1]\n",
    "    transforms.Lambda(lambda x: x.view(-1)) #flatten to (784,)\n",
    "])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Load training dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, transform=transform, download=True\n",
    ")\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of object 1: <class 'torch.Tensor'>\n",
      "size of tensor:  torch.Size([32, 784])\n",
      "type of object 2: <class 'torch.Tensor'>\n",
      "size of tensor:  torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    for i, thing in enumerate(batch):\n",
    "        print(f\"type of object {i+1}: {type(thing)}\")\n",
    "        if type(thing) == torch.Tensor:\n",
    "            print(f\"size of tensor: \", thing.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the dataloaders have the first item as a tensor containing the images, while the second item is the digit. Both are tensors, and the first dimension is used for the batch. The final two dimensions are for the x and y coordinate for the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def image_reshape(images):\n",
    "    \"\"\"\n",
    "    Function to resize images from the dataloaders.\n",
    "\n",
    "    Inputs:\n",
    "    - images: tensor containing a batch of images.\n",
    "    Shape: (batch_size, num_observed_vars)\n",
    "\n",
    "    Outputs:\n",
    "    - images: tensor containing a batch of resized images.\n",
    "    Shape: (batch_size, image_height, image_width, image_channels)\n",
    "    \"\"\"\n",
    "\n",
    "    images = images.reshape((-1, 1, 28, 28))\n",
    "    return images\n",
    "\n",
    "for (images, digits) in train_loader:\n",
    "    images = image_reshape(images)\n",
    "    print(images.shape)\n",
    "    plt.imshow(images[0, 0, :, :], cmap='gray')\n",
    "    plt.title(\"28x28 Tensor Image\")\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F1ZcpDhMIrSt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functions generated\n"
     ]
    }
   ],
   "source": [
    "def diag_normal_std_normal_KL(mu, sigma):\n",
    "  \"\"\"\n",
    "  Function to compute KL divergence between the given diagonal normal distribution and the standard normal\n",
    "  distribution, N(0, I). This is equivalent to multivariate_normal_KL_divergence(), except it is\n",
    "  more efficient when the second distribution is the standard normal. Closed form is obtained from\n",
    "  https://arxiv.org/pdf/1606.05908.\n",
    "\n",
    "  This function is vectorised.\n",
    "\n",
    "  Inputs:\n",
    "  - mu: a vector of means for the distribution.\n",
    "  Shape: (*, n_vars)\n",
    "  - sigma: a vector of covariance diagonals for the distribution.\n",
    "  Shape: (*, n_vars)\n",
    "\n",
    "  Outputs:\n",
    "  - KL_divergence: computed KL divergence, D(distribution_1, N(0, I)).\n",
    "  Shape: (*)\n",
    "  \"\"\"\n",
    "\n",
    "  #Initialise variables\n",
    "  k = mu.shape[-1]\n",
    "\n",
    "  #Compute traces and log determinants (det of diagonal is product of diagonals)\n",
    "  sigma_traces = sigma.sum(-1)\n",
    "  sigma_log_det = torch.sum(torch.log(sigma), dim = -1)\n",
    "\n",
    "  #Compute KL-divergence\n",
    "  KL_divergence = 1/2 * ( sigma_traces + torch.sum(mu ** 2, dim = -1) - k - sigma_log_det )\n",
    "\n",
    "  return KL_divergence\n",
    "\n",
    "\n",
    "def loss_per_batch(X, z, Q, P):\n",
    "  \"\"\"\n",
    "  Function to compute loss over entire batch. In a VAE, this is the negative of the evidence lower bound (ELBO).\n",
    "  We wish to maximise the ELBO, hence we minimise the negative ELBO. \n",
    "  The negative ELBO is given by KL_divergence - P_log_likelihood.\n",
    "\n",
    "  Note that we assume that the latent variables z are drawn from a multivariate standard normal \n",
    "  distribution, N(0, I).\n",
    "\n",
    "  Here, \"P\" refers to our map from latent variables to data points, P(X|z).\n",
    "\n",
    "  Inputs:\n",
    "  - X: tensor containing the observed data points for a batch.\n",
    "  Shape: (batch_size, n_observed_vars)\n",
    "  - z: tensor containing the latent variables corresponding to the given observed data points.\n",
    "  Note that we allow multiple latent variable samples for a single data point, as z_batch_size \n",
    "  does not need to be 1.\n",
    "  Shape: (batch_size, z_batch_size, n_latent_vars)\n",
    "  - Q: normal distribution for Q(z|X).\n",
    "  Shape: (batch_size, n_latent_vars)\n",
    "  - P: normal distribution for P(X|z).\n",
    "  Shape: (batch_size, z_batch_size, n_observed_vars)\n",
    "\n",
    "  Note that Q and P are normal distributions objects, not multivariate normal objects. This is\n",
    "  because we assume that the covariance matrices are diagonal.\n",
    "\n",
    "  Outputs:\n",
    "  - P_log_likelihood: log likelihood of the observed data points given the latent variables.\n",
    "  Shape: (batch_size)\n",
    "  - KL_divergence: KL divergence between the given Q(z|X) and the standard normal distribution N(0, I).\n",
    "  Shape: (batch_size)\n",
    "\n",
    "  Note that the ELBO (which is a single value for loss) is KL_divergence - P_log_likelihood.\n",
    "  \"\"\"\n",
    "\n",
    "  #Initialise variables\n",
    "  #n_observed = X.shape[-1]\n",
    "  #n_latent = z.shape[-1]\n",
    "  z_batch_size = z.shape[1]\n",
    "  #batch_size = z.shape[0]\n",
    "\n",
    "  #Compute loss per data point\n",
    "  #Note that Pytorch should automatically implement the reparametrisation trick here.\n",
    "  P_log_likelihood = P.log_prob(X.unsqueeze(1).repeat(1, z_batch_size, 1))\n",
    "  #Sum over observed variables and average over latent variables\n",
    "  P_log_likelihood = torch.mean(torch.sum(P_log_likelihood, dim = 2), dim = 1)\n",
    "  KL_divergence = diag_normal_std_normal_KL(Q.loc, Q.scale)\n",
    "\n",
    "  return P_log_likelihood, KL_divergence\n",
    "\n",
    "print(\"All functions generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for the Functions Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "normal_KL_divergence_from_standard_normal test.\n",
      "Expected value==Test Value: True.\n",
      "Manually-calculated value == Expected Value: True.\n",
      "Manually-calculated value == Test Value: True\n",
      "\n",
      "\n",
      "loss_per_batch test.\n",
      "Expected_value == Obtained_value: True.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kperc\\AppData\\Local\\Temp\\ipykernel_17528\\4024282803.py:24: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4416.)\n",
      "  manual_KL[i] = 1/2 * (torch.log(torch.linalg.det(Z_sigma) / torch.linalg.det(Q_sigma[i, :, :])) - n_latent + torch.trace(torch.linalg.inv(Z_sigma) @ Q_sigma[i, :, :]) + (Z_mu - Q_mu[i, :]).T @ torch.linalg.inv(Z_sigma) @ (Z_mu - Q_mu[i, :]))\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "n_observed = 10\n",
    "n_latent = 5\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "Z_mu = torch.zeros(n_latent)\n",
    "Z_sigma = torch.eye(n_latent)\n",
    "Z = torch.distributions.MultivariateNormal(Z_mu, Z_sigma)\n",
    "\n",
    "Q_mu = Z.sample((batch_size,))\n",
    "Q_sigma = torch.abs(Z.sample((batch_size,)))\n",
    "\n",
    "#Compute KL-divergence between Q and I manually\n",
    "Q = torch.distributions.Normal(Q_mu, Q_sigma)\n",
    "test_KL = diag_normal_std_normal_KL(Q_mu, Q_sigma)\n",
    "Q_sigma = torch.diag_embed(Q_sigma)\n",
    "Q_multi = torch.distributions.MultivariateNormal(Q_mu, Q_sigma)\n",
    "expected_KL = torch.distributions.kl_divergence(Q_multi, Z)\n",
    "\n",
    "manual_KL = torch.zeros((batch_size,))\n",
    "for i in range(batch_size):\n",
    "    manual_KL[i] = 1/2 * (torch.log(torch.linalg.det(Z_sigma) / torch.linalg.det(Q_sigma[i, :, :])) - n_latent + torch.trace(torch.linalg.inv(Z_sigma) @ Q_sigma[i, :, :]) + (Z_mu - Q_mu[i, :]).T @ torch.linalg.inv(Z_sigma) @ (Z_mu - Q_mu[i, :]))\n",
    "\n",
    "#Test 1: normal_KL_divergence_from_standard_normal()\n",
    "print(f\"\\n\\nnormal_KL_divergence_from_standard_normal test.\\nExpected value==Test Value: {torch.allclose(expected_KL, test_KL, atol=1e-5, rtol=1e-3)}.\\nManually-calculated value == Expected Value: {torch.allclose(manual_KL, expected_KL, atol=1e-5, rtol=1e-3)}.\\nManually-calculated value == Test Value: {torch.allclose(manual_KL, test_KL, atol=1e-5, rtol=1e-3)}\")\n",
    "#print(\"manual_KL: \", manual_KL)\n",
    "#Test passed\n",
    "\n",
    "#Initialise variables\n",
    "z_batch_size = 3\n",
    "\n",
    "#Create new distribution, P=P(X|z)\n",
    "Z_mu = torch.zeros(n_observed)\n",
    "Z_sigma = torch.eye(n_observed)\n",
    "Z = torch.distributions.MultivariateNormal(Z_mu, Z_sigma)\n",
    "P_mu = Z.sample((batch_size, z_batch_size))\n",
    "P_sigma = torch.abs(Z.sample((batch_size, z_batch_size)))\n",
    "P = torch.distributions.Normal(P_mu, P_sigma)\n",
    "manual_P_log_likelihood = torch.zeros((batch_size, z_batch_size, n_latent))\n",
    "\n",
    "#Sample a single batch of X\n",
    "#In practice this would be conditioned on a sample of z from Q\n",
    "X = P.sample((1,)).squeeze()[:, 0, :]\n",
    "Z_mu = torch.zeros(n_latent)\n",
    "Z_sigma = torch.eye(n_latent)\n",
    "Z = torch.distributions.MultivariateNormal(Z_mu, Z_sigma)\n",
    "\n",
    "#Sample z from Q, then compute log-likelihood of X given z\n",
    "z = Q.rsample((z_batch_size,))\n",
    "z = z.permute(1, 0, 2)  # Shape: (batch_size, z_batch_size, n_latent_vars)\n",
    "manual_P_log_likelihood = P.log_prob(X.unsqueeze(1).repeat(1, z_batch_size, 1))\n",
    "manual_P_log_likelihood = torch.mean(torch.sum(manual_P_log_likelihood, dim=2), dim = 1)\n",
    "manual_KL_divergence = torch.distributions.kl_divergence(Q_multi, Z)\n",
    "\n",
    "#Average over all z samples\n",
    "manual_loss = -manual_P_log_likelihood + manual_KL_divergence\n",
    "test_P_log_likelihood, test_KL_divergence = loss_per_batch(X, z, Q, P)\n",
    "test_loss = -test_P_log_likelihood + test_KL_divergence\n",
    "\n",
    "#TEST 2: loss_per_batch\n",
    "print(f\"\\n\\nloss_per_batch test.\\nExpected_value == Obtained_value: {torch.allclose(manual_loss, test_loss, atol=1e-5, rtol=1e-3)}.\")\n",
    "#Test passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We will use Optuna to find the optimal set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(layer):\n",
    "    \"\"\"\n",
    "    Function to initialise weights of a layer.\n",
    "\n",
    "    Inputs:\n",
    "    - layer: a layer of the neural network.\n",
    "\n",
    "    Note that there are no outputs, as the weights are initialised in place.\n",
    "    \"\"\"\n",
    "\n",
    "    #Only update parameters if the layer is a linear layer\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        #Initialise weights using Xavier uniform distribution and biases to zero\n",
    "        torch.nn.init.xavier_uniform_(layer.weight)\n",
    "        torch.nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    return None\n",
    "\n",
    "#Create Neural Network Model\n",
    "class neural_network(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Create class for a neural network with a variable number of layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_sizes, n_inputs, n_outputs, tanh_output, dropout_rate = 0.5):\n",
    "        \"\"\"\n",
    "        Initialise the model and relevant hyperparameters\n",
    "\n",
    "        Inputs:\n",
    "        - hidden_sizes: a list containing the number of neurons in each layer in the neural network.\n",
    "        - n_inputs: the number of inputs to be produced by the neural network.\n",
    "        - n_outputs: the number of outputs to be produced by the neural network.\n",
    "        - logistic_output: a boolean which determines whether the output layer should use a logistic \n",
    "        activation function.\n",
    "        - dropout_rate: the dropout rate to be applied after each layer.\n",
    "        \"\"\"\n",
    "\n",
    "        super(neural_network, self).__init__()\n",
    "\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.tanh_output = tanh_output\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        #Create a ModuleList to hold the layers and dropout rates\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.dropouts = torch.nn.ModuleList()\n",
    "\n",
    "        #Create layers of neural network\n",
    "        in_size = n_inputs\n",
    "        for out_size in hidden_sizes:\n",
    "            self.layers.append(torch.nn.Linear(in_size, out_size))\n",
    "            self.dropouts.append(torch.nn.Dropout(p=dropout_rate))\n",
    "\n",
    "            #Update number of inputs for next layer\n",
    "            in_size = out_size\n",
    "\n",
    "        #Add final layer to produce outputs\n",
    "        #Note that we do not apply dropout to the output layer.\n",
    "        self.layers.append(torch.nn.Linear(in_size, n_outputs))\n",
    "\n",
    "        #Initialise weights of each layer\n",
    "        self.layers.apply(init_weights)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\" \n",
    "        Establish how inputs move through the neural network. This is where we set the \n",
    "        activation function.\n",
    "\n",
    "        Inputs:\n",
    "        - X: tensor of inputs.\n",
    "        Shape: (batch_size, n_inputs)\n",
    "\n",
    "        Outputs:\n",
    "        - X: tensor of outputs. If self.logistic_output is True, then the elements are between 0 and \n",
    "        1, else they can take any real value.\n",
    "        Shape: (batch_size, n_outputs)\n",
    "        \"\"\"\n",
    "\n",
    "        #Apply each layer in the neural network\n",
    "        #We apply the final layer separately, as we do not apply dropout.\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            X = torch.nn.functional.relu(layer(X))\n",
    "            X = self.dropouts[i](X)\n",
    "        # Apply output layer (no activation or dropout)\n",
    "        X = self.layers[-1](X)\n",
    "\n",
    "        #Apply logistic function to outputs if specified\n",
    "        if self.tanh_output == True:\n",
    "            X = torch.tanh(X)\n",
    "\n",
    "        #Error handling: ensure that outputs don't contain NaN or Inf values\n",
    "        if torch.isnan(X).any() or torch.isinf(X).any():\n",
    "            raise ValueError(\"Neural network output contains NaN or Inf values.\")\n",
    "\n",
    "        return X\n",
    "\n",
    "class VAE(torch.nn.Module):\n",
    "    \"\"\"      \n",
    "    Create VAE model. The main goal is to estimate the following with neural networks:\n",
    "    - f = f(z; theta): this is a map from latent variables to a point estimate for X, as we assume that\n",
    "    P(X|z) has distribution N( f(z; theta), sigma^2*I ). \n",
    "    - mu = mu(X; phi) and sigma = sigma(X; phi): these parametrise Q(z|X), which we assume to have \n",
    "    distribution N( mu(X; phi), sigma(X; phi) ).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observed, n_latent, f_hidden_sizes, Q_hidden_sizes, z_batch_size):\n",
    "        \"\"\"\n",
    "        Initialise the model and relevant hyperparameters\n",
    "\n",
    "        Inputs:\n",
    "        - n_observed: the number of observed variables\n",
    "        - n_latent: the number of latent variables\n",
    "        - f_hidden_sizes: a list for the number of hidden sizes in the neural network f(z; theta).\n",
    "        - Q_hidden_sizes: a list for the number of hidden sizes in the neural network for \n",
    "        the parameters of Q (mu(X; phi) and sigma(X; phi)).\n",
    "        - z_batch_size: the number of latent variables to sample per observed data point.\n",
    "\n",
    "        Notes:\n",
    "        - self.n_outputs depends on the number of latent and observed variables. \n",
    "        - self.log_sigma2 is a parameter of the model, as it is not specific to each data point\n",
    "        (homoscedastic variance). We use log variance, as then taking the exponential ensures varaince\n",
    "        is positive.\n",
    "        \"\"\"\n",
    "\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        #Initialise hyperparameters\n",
    "        self.n_observed = n_observed\n",
    "        self.n_latent = n_latent\n",
    "        self.z_batch_size = z_batch_size\n",
    "\n",
    "        #Initialise log variance to a small value to avoid posterior collapse.\n",
    "        #Note that this will be optimised globally (a single value for all data points).\n",
    "        self.log_sigma2 = torch.nn.Parameter(torch.tensor([-4.0]))\n",
    "\n",
    "        #Initialise instances of neural networks.\n",
    "        #Recall that f maps z to X and Q maps X to z, so this determines the number of inputs and outputs.\n",
    "        #For Q, mu and sigma both have n_latent elements, hence the total outputs is 2*n_latent.\n",
    "        self.f_nn = neural_network(f_hidden_sizes, n_latent, n_observed, True)\n",
    "        self.Q_n_outputs = 2*n_latent\n",
    "        self.Q_nn = neural_network(Q_hidden_sizes, n_observed, self.Q_n_outputs, False)\n",
    "\n",
    "    def forward(self, inputs, skip_encode = False):\n",
    "        \"\"\"\n",
    "        Forward function for the VAE model. Here, we use Q to encode our data, then f to decode.\n",
    "\n",
    "        Inputs:\n",
    "        - inputs: depends on skip_encode (see below).\n",
    "        - skip_encode: a boolean which determines whether \"inputs\" are observed data points or latent\n",
    "        variables.\n",
    "        if skip_encode == False:\n",
    "            inputs is a tensor of observed data.\n",
    "            Shape: (batch_size, n_observed)\n",
    "        if skip_encode == True:\n",
    "            inputs is a tensor of latent variables.\n",
    "            Shape: (batch_size,  z_batch_size, n_latent)\n",
    "\n",
    "        Outputs:\n",
    "        - outputs: a tuple, where the elements depends on skip_encode.\n",
    "        if skip_encode == False:\n",
    "            - z: samples from Q, which represent likely latent variables corresponding to each observed\n",
    "            data point.\n",
    "            Shape: (batch_size, z_batch_size, n_latent)\n",
    "            - Q: distribution for Q(z|X), which is a normal distribution.\n",
    "            Shape: (batch_size, n_latent).\n",
    "            - P: distribution for P(X|z), which is a normal distribution.\n",
    "            Shape: (batch_size, z_batch_size, n_observed).\n",
    "        if skip_encode == True:\n",
    "            - P: see above.\n",
    "\n",
    "        Note that point estimates for the reconstructed data is given by the mean of P (=f(z; theta)).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #Initialise variables\n",
    "        batch_size = inputs.shape[0]\n",
    "\n",
    "        if skip_encode == True:\n",
    "            #Relabel inputs\n",
    "            z = inputs\n",
    "\n",
    "        else:\n",
    "            #Relabel inputs\n",
    "            X = inputs\n",
    "\n",
    "            #Encode data as probability distributions\n",
    "            #Elements of variance matrix must be positive, so take exponential.\n",
    "            Q_parameters = self.Q_nn(X)\n",
    "            Q_mu = Q_parameters[:, 0:self.n_latent]\n",
    "            #Clamp values to avoid numerical issues before taking exponential\n",
    "            Q_sigma = torch.exp(torch.clamp(Q_parameters[:, self.n_latent:], min=-10, max=10))\n",
    "            Q = torch.distributions.Normal(Q_mu, Q_sigma)\n",
    "\n",
    "            #Sample from Q to get encoded variables.\n",
    "            #Permute dimensions so that z has the intended shape: (batch_size, z_batch_size, n_latent)\n",
    "            z = Q.rsample((self.z_batch_size,))\n",
    "            z = torch.movedim(z, 0, 1) \n",
    "\n",
    "        #Decode sampled latent variables.\n",
    "        #Here, f is the point estimate for X, i.e. it is our reconstructed observed data.\n",
    "        f = self.f_nn(z)\n",
    "        P_mu = f\n",
    "        #Clamp log_sigma2 to avoid numerical issues before taking exponential\n",
    "        sigma2 = torch.exp(torch.clamp(self.log_sigma2, min=-10, max=10))\n",
    "        P_sigma = sigma2 * torch.ones((batch_size, self.z_batch_size, self.n_observed))\n",
    "        P = torch.distributions.Normal(P_mu, P_sigma)\n",
    "\n",
    "        #Outputs depend on whether we skipped encoding or not\n",
    "        if skip_encode == True:\n",
    "            outputs = ( P )\n",
    "        else: \n",
    "            outputs = ( z, Q, P )\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test function for VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_mu.shape: torch.Size([32, 5]), Q_sigma.shape: torch.Size([32, 5]), z.shape: torch.Size([32, 7, 5]), P_mu.shape: torch.Size([32, 7, 784]), P_sigma.shape: torch.Size([32, 7, 784])\n",
      "P_mu.shape: torch.Size([32, 7, 784]), P_sigma.shape: torch.Size([32, 7, 784])\n"
     ]
    }
   ],
   "source": [
    "#Extract a single batch\n",
    "for images, digits in train_loader:\n",
    "    break\n",
    "\n",
    "#Initialise hyperarameters\n",
    "n_observed = images.shape[-1]\n",
    "n_latent = 5\n",
    "f_hidden_sizes = [10, 12]\n",
    "Q_hidden_sizes = [20, 24, 28]\n",
    "sigma = 1\n",
    "z_batch_size = 7\n",
    "\n",
    "test_model = VAE(n_observed, n_latent, f_hidden_sizes, Q_hidden_sizes, z_batch_size)\n",
    "\n",
    "#Test full model\n",
    "z, Q, P = test_model(images, skip_encode = False)\n",
    "print(f\"Q_mu.shape: {Q.loc.shape}, Q_sigma.shape: {Q.scale.shape}, z.shape: {z.shape}, P_mu.shape: {P.loc.shape}, P_sigma.shape: {P.scale.shape}\")\n",
    "\n",
    "#Test only decoder\n",
    "z = torch.distributions.MultivariateNormal(torch.zeros(n_latent), torch.eye(n_latent)).sample((batch_size, z_batch_size))\n",
    "P = test_model(z, skip_encode = True)\n",
    "print(f\"P_mu.shape: {P.loc.shape}, P_sigma.shape: {P.scale.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Functions for VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kperc\\VAE\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def train_VAE(train_loader, test_loader, model, n_epochs, optimiser, print_flag = False):\n",
    "  \"\"\"\n",
    "  Function to train the VAE.\n",
    "\n",
    "  Inputs:\n",
    "  - train_loader: the `DataLoader` object for the training dataset.\n",
    "  - test_loader: the `DataLoader` object for the testing dataset.\n",
    "  - model: the VAE model to be trained.\n",
    "  - n_epochs: number of times to loop through entire dataset during optimisation.\n",
    "  - print_flag: a boolean which determines whether to print training statistics.\n",
    "\n",
    "  Outputs:\n",
    "  - model: trained VAE model.\n",
    "  - epoch_train_losses: tensor of loss per epoch over the training dataset.\n",
    "  Shape: (n_epochs,)\n",
    "  - epoch_test_losses: tensor of loss per epoch over the testing dataset.\n",
    "  Shape: (n_epochs,)\n",
    "  \"\"\"\n",
    "  \n",
    "  #Set model to training mode\n",
    "  model.train()\n",
    "\n",
    "  #Initialise variables\n",
    "  n_batches = len(train_loader)\n",
    "  batch_sizes = [len(batch) for batch in train_loader]\n",
    "  n_checkpoints = 3\n",
    "  train_size = len(train_loader.dataset)\n",
    "  #Only want to print n_checkpoints times per epoch\n",
    "  checkpoints = [torch.floor(torch.tensor((n_batches-1)/n_checkpoints * (i))) for i in range(n_checkpoints+1)]\n",
    "  i = 0\n",
    "  epoch_train_losses = torch.zeros((n_epochs,))\n",
    "  epoch_test_losses = torch.zeros((n_epochs,))\n",
    "\n",
    "  #Loop through entire dataset the specified number of times\n",
    "  for epoch_index in range(n_epochs):\n",
    "    #Keep track of epoch P_log_likelihood and KL_divergence - these are used to compute loss\n",
    "    train_P_log_likelihood = 0\n",
    "    train_KL_divergence = 0\n",
    "\n",
    "    #Initialise beta for annealing. Start close to 0, increase linearly to 1.0.\n",
    "    Beta = 1.0 * (epoch_index) / (n_epochs-1)\n",
    "\n",
    "    for batch_index, (images, digits) in enumerate(train_loader):\n",
    "      #Compute loss\n",
    "      #Note that we use annealing for batch_loss, as this is used for the optimiser\n",
    "      z, Q, P = model(images, skip_encode = False)\n",
    "      batch_P_log_likelihood, batch_KL_divergence = loss_per_batch(images, z, Q, P)\n",
    "      batch_P_log_likelihood = torch.sum(batch_P_log_likelihood)\n",
    "      batch_KL_divergence = torch.sum(batch_KL_divergence)\n",
    "      batch_loss = Beta * batch_KL_divergence - batch_P_log_likelihood\n",
    "      train_P_log_likelihood += batch_P_log_likelihood/train_size\n",
    "      train_KL_divergence += batch_KL_divergence/train_size\n",
    "      \n",
    "      #Update parameters based on optimiser\n",
    "      optimiser.zero_grad()\n",
    "      batch_loss.backward()\n",
    "      optimiser.step()\n",
    "\n",
    "      #Only print occasional batch statistics\n",
    "      if (print_flag == True) and (batch_index == checkpoints[i]):\n",
    "        current_batch_size = batch_sizes[batch_index]\n",
    "        print(f\"Batch number: {batch_index+1}/{n_batches}. Average P_log_likelihood: {batch_P_log_likelihood/current_batch_size}. Average KL_divergence: {batch_KL_divergence/current_batch_size}.\")\n",
    "        i += 1\n",
    "        \n",
    "    #Reset i for next epoch\n",
    "    i = 0\n",
    "\n",
    "    #Store epoch losses. Note that we do not use annealing to stay consistent\n",
    "    epoch_train_losses[epoch_index] = -train_P_log_likelihood + train_KL_divergence\n",
    "    test_P_log_likelihood, test_KL_divergence = test_VAE(test_loader, model)\n",
    "    epoch_test_losses[epoch_index] = -test_P_log_likelihood + test_KL_divergence\n",
    "\n",
    "    #Print epoch statistics\n",
    "    if print_flag == True:\n",
    "      print(f\"\\nEpoch number: {epoch_index+1}/{n_epochs}.\")\n",
    "      print(f\"Train loss: {epoch_train_losses[epoch_index]}, Train P_log_likelihood: {train_P_log_likelihood}, Train KL_divergence: {train_KL_divergence}.\")\n",
    "      print(f\"Test loss: {epoch_test_losses[epoch_index]}, Test P_log_likelihood: {test_P_log_likelihood}, Test KL_divergence: {test_KL_divergence}.\\n\")\n",
    "\n",
    "  return model, epoch_train_losses, epoch_test_losses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_VAE(test_loader, model):\n",
    "  \"\"\"\n",
    "  Function to test the VAE.\n",
    "\n",
    "  Inputs:\n",
    "  - test_loader: the `DataLoader` object for the testing dataset.\n",
    "  - model: the VAE model to be trained.\n",
    "\n",
    "  Outputs:\n",
    "  - loss: total training loss for the model\n",
    "  \"\"\"\n",
    "\n",
    "  #Set model to evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "  #Initialise variables\n",
    "  P_log_likelihood = 0\n",
    "  KL_divergence = 0\n",
    "  test_size = len(test_loader.dataset)\n",
    "\n",
    "  #Loop through dataset\n",
    "  for batch_index, (images, digits) in enumerate(test_loader):      \n",
    "      #Compute loss\n",
    "      z, Q, P = model(images, skip_encode = False)\n",
    "      batch_P_log_likelihood, batch_KL_divergence = loss_per_batch(images, z, Q, P)\n",
    "      P_log_likelihood += torch.sum(batch_P_log_likelihood)/test_size\n",
    "      KL_divergence += torch.sum(batch_KL_divergence)/test_size\n",
    "\n",
    "  return P_log_likelihood, KL_divergence\n",
    "\n",
    "\n",
    "def objective(trial, train_loader, test_loader, z_batch_size, n_epochs):\n",
    "  \"\"\"  \n",
    "  Objective function for Optuna to optimise. Here, hyperparameters are suggested by the package.\n",
    "  \n",
    "  Inputs:\n",
    "  - trial: optuna.trial.Trial object\n",
    "  - train_loader & test_loader: dataloaders for the training and testing datasets.\n",
    "  - z_batch_size: number of latent samples per data point to compute gradient.\n",
    "  - n_epochs: number of times to loop through entire dataset.\n",
    "\n",
    "  Outputs:\n",
    "  - loss: scalar value for the loss across the entire test dataset.\n",
    "  \"\"\"\n",
    "\n",
    "  #Suggest hyperparameters\n",
    "  n_latent = trial.suggest_int(\"n_latent\", 5, 25)\n",
    "  f_n_layers = trial.suggest_int(\"f_n_layers\", 1, 3)\n",
    "  Q_n_layers = trial.suggest_int(\"Q_n_layers\", 1, 3)\n",
    "  #We will start with all layers having equal size\n",
    "  f_hidden_size = trial.suggest_int(\"f_hidden_size\", 8, 64)\n",
    "  Q_hidden_size = trial.suggest_int(\"Q_hidden_size\", 128, 1024)\n",
    "  learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True)\n",
    "  #dropout_rate = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "  #Initialise variables and other hyperparameters\n",
    "  f_hidden_sizes = [f_hidden_size for _ in range(f_n_layers)]\n",
    "  Q_hidden_sizes = [Q_hidden_size for _ in range(Q_n_layers)]\n",
    "  for (images, digits) in train_loader:\n",
    "    break\n",
    "  #Number of observed variables is product of all dimension lengths except first (batch)\n",
    "  n_observed = torch.prod(torch.tensor(images[(0,) + (slice(None),) * (images.dim() - 1)].shape))\n",
    "\n",
    "  #Initialise model and optimiser\n",
    "  model = VAE(n_observed, n_latent, f_hidden_sizes, Q_hidden_sizes, z_batch_size)\n",
    "  #model = torch.compile(model)\n",
    "  optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  #Train model\n",
    "  #Add error handling in the case that NaN or Inf values are produced\n",
    "  try:\n",
    "     model, _, epoch_test_losses = train_VAE(train_loader, test_loader, model, n_epochs, optimiser, print_flag = False)\n",
    "  except ValueError as e:\n",
    "      print(f\"Error during training: {e}\")\n",
    "      raise optuna.TrialPruned()\n",
    "  \n",
    "  #Extract final test loss\n",
    "  loss = epoch_test_losses[-1]\n",
    "\n",
    "  return loss\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-12 09:26:55,041] Using an existing study with name 'VAE MNIST Optuna Study' instead of creating a new one.\n",
      "[I 2025-06-12 09:44:15,559] Trial 20 finished with value: 274.4784240722656 and parameters: {'n_latent': 5, 'f_n_layers': 2, 'Q_n_layers': 2, 'f_hidden_size': 57, 'Q_hidden_size': 933, 'learning_rate': 0.0001087035273915489}. Best is trial 11 with value: -150.87220764160156.\n",
      "[I 2025-06-12 09:53:31,986] Trial 21 finished with value: -30.116098403930664 and parameters: {'n_latent': 17, 'f_n_layers': 2, 'Q_n_layers': 3, 'f_hidden_size': 53, 'Q_hidden_size': 172, 'learning_rate': 0.0007156565589641785}. Best is trial 11 with value: -150.87220764160156.\n",
      "[I 2025-06-12 10:19:05,549] Trial 22 finished with value: -34.993045806884766 and parameters: {'n_latent': 23, 'f_n_layers': 3, 'Q_n_layers': 3, 'f_hidden_size': 48, 'Q_hidden_size': 783, 'learning_rate': 0.0027947199437317687}. Best is trial 11 with value: -150.87220764160156.\n",
      "[I 2025-06-12 10:41:08,422] Trial 23 finished with value: -151.33316040039062 and parameters: {'n_latent': 25, 'f_n_layers': 2, 'Q_n_layers': 3, 'f_hidden_size': 41, 'Q_hidden_size': 663, 'learning_rate': 0.0008890474890498897}. Best is trial 23 with value: -151.33316040039062.\n",
      "[I 2025-06-12 11:01:12,019] Trial 24 finished with value: -128.9656524658203 and parameters: {'n_latent': 23, 'f_n_layers': 2, 'Q_n_layers': 3, 'f_hidden_size': 40, 'Q_hidden_size': 652, 'learning_rate': 0.0008582618455612497}. Best is trial 23 with value: -151.33316040039062.\n",
      "[I 2025-06-12 11:18:22,497] Trial 25 finished with value: -43.94864273071289 and parameters: {'n_latent': 23, 'f_n_layers': 2, 'Q_n_layers': 2, 'f_hidden_size': 39, 'Q_hidden_size': 617, 'learning_rate': 0.0033061011866878055}. Best is trial 23 with value: -151.33316040039062.\n",
      "[I 2025-06-12 11:34:39,240] Trial 26 finished with value: 76.83332824707031 and parameters: {'n_latent': 14, 'f_n_layers': 2, 'Q_n_layers': 3, 'f_hidden_size': 24, 'Q_hidden_size': 447, 'learning_rate': 0.0008795150088631292}. Best is trial 23 with value: -151.33316040039062.\n",
      "[I 2025-06-12 12:02:29,465] Trial 27 finished with value: -98.62699890136719 and parameters: {'n_latent': 24, 'f_n_layers': 2, 'Q_n_layers': 3, 'f_hidden_size': 40, 'Q_hidden_size': 875, 'learning_rate': 0.0022450052991932344}. Best is trial 23 with value: -151.33316040039062.\n",
      "[I 2025-06-12 12:19:10,669] Trial 28 finished with value: -57.5161018371582 and parameters: {'n_latent': 19, 'f_n_layers': 1, 'Q_n_layers': 3, 'f_hidden_size': 37, 'Q_hidden_size': 653, 'learning_rate': 0.00025481708942140936}. Best is trial 23 with value: -151.33316040039062.\n",
      "[I 2025-06-12 12:30:46,723] Trial 29 finished with value: 110.76122283935547 and parameters: {'n_latent': 23, 'f_n_layers': 2, 'Q_n_layers': 2, 'f_hidden_size': 49, 'Q_hidden_size': 263, 'learning_rate': 0.007998563223214943}. Best is trial 23 with value: -151.33316040039062.\n",
      "[I 2025-06-12 12:53:00,161] Trial 30 finished with value: -97.22529602050781 and parameters: {'n_latent': 21, 'f_n_layers': 2, 'Q_n_layers': 3, 'f_hidden_size': 43, 'Q_hidden_size': 818, 'learning_rate': 0.0003940490412482863}. Best is trial 23 with value: -151.33316040039062.\n",
      "[I 2025-06-12 13:12:59,778] Trial 31 finished with value: -122.18716430664062 and parameters: {'n_latent': 24, 'f_n_layers': 2, 'Q_n_layers': 3, 'f_hidden_size': 36, 'Q_hidden_size': 641, 'learning_rate': 0.0011004056353945012}. Best is trial 23 with value: -151.33316040039062.\n",
      "[I 2025-06-12 13:35:04,112] Trial 32 finished with value: -128.36106872558594 and parameters: {'n_latent': 21, 'f_n_layers': 2, 'Q_n_layers': 3, 'f_hidden_size': 59, 'Q_hidden_size': 731, 'learning_rate': 0.000845456336957906}. Best is trial 23 with value: -151.33316040039062.\n",
      "[I 2025-06-12 13:57:30,896] Trial 33 finished with value: -182.57762145996094 and parameters: {'n_latent': 25, 'f_n_layers': 2, 'Q_n_layers': 3, 'f_hidden_size': 59, 'Q_hidden_size': 700, 'learning_rate': 0.0005396942404635338}. Best is trial 33 with value: -182.57762145996094.\n",
      "[I 2025-06-12 14:15:10,446] Trial 34 finished with value: -108.69882202148438 and parameters: {'n_latent': 25, 'f_n_layers': 2, 'Q_n_layers': 3, 'f_hidden_size': 28, 'Q_hidden_size': 513, 'learning_rate': 0.0005544848288222122}. Best is trial 33 with value: -182.57762145996094.\n",
      "[I 2025-06-12 14:59:15,031] Trial 35 finished with value: -135.9351348876953 and parameters: {'n_latent': 23, 'f_n_layers': 2, 'Q_n_layers': 3, 'f_hidden_size': 47, 'Q_hidden_size': 826, 'learning_rate': 0.0005879107522621881}. Best is trial 33 with value: -182.57762145996094.\n",
      "[I 2025-06-12 15:39:02,854] Trial 36 finished with value: -181.58592224121094 and parameters: {'n_latent': 24, 'f_n_layers': 1, 'Q_n_layers': 3, 'f_hidden_size': 49, 'Q_hidden_size': 910, 'learning_rate': 0.00035329196624217464}. Best is trial 33 with value: -182.57762145996094.\n",
      "[I 2025-06-12 16:51:02,410] Trial 37 finished with value: -195.24118041992188 and parameters: {'n_latent': 25, 'f_n_layers': 1, 'Q_n_layers': 3, 'f_hidden_size': 51, 'Q_hidden_size': 941, 'learning_rate': 0.0002915168436070506}. Best is trial 37 with value: -195.24118041992188.\n",
      "[I 2025-06-12 17:20:42,288] Trial 38 finished with value: 116.7973403930664 and parameters: {'n_latent': 10, 'f_n_layers': 1, 'Q_n_layers': 3, 'f_hidden_size': 60, 'Q_hidden_size': 987, 'learning_rate': 0.00015745686258419075}. Best is trial 37 with value: -195.24118041992188.\n",
      "[I 2025-06-12 18:18:56,974] Trial 39 finished with value: 48.61054992675781 and parameters: {'n_latent': 12, 'f_n_layers': 1, 'Q_n_layers': 2, 'f_hidden_size': 52, 'Q_hidden_size': 932, 'learning_rate': 0.0003261895242421463}. Best is trial 37 with value: -195.24118041992188.\n",
      "[I 2025-06-12 18:37:50,783] Trial 40 finished with value: -128.81655883789062 and parameters: {'n_latent': 22, 'f_n_layers': 1, 'Q_n_layers': 2, 'f_hidden_size': 56, 'Q_hidden_size': 951, 'learning_rate': 0.0001978721821984061}. Best is trial 37 with value: -195.24118041992188.\n",
      "[I 2025-06-12 19:03:33,171] Trial 41 finished with value: -181.8804473876953 and parameters: {'n_latent': 24, 'f_n_layers': 1, 'Q_n_layers': 3, 'f_hidden_size': 50, 'Q_hidden_size': 894, 'learning_rate': 0.0003670694937021769}. Best is trial 37 with value: -195.24118041992188.\n",
      "[I 2025-06-12 19:33:40,612] Trial 42 finished with value: -187.4493408203125 and parameters: {'n_latent': 24, 'f_n_layers': 1, 'Q_n_layers': 3, 'f_hidden_size': 51, 'Q_hidden_size': 977, 'learning_rate': 0.0004449462116156793}. Best is trial 37 with value: -195.24118041992188.\n",
      "[I 2025-06-12 20:01:13,859] Trial 43 finished with value: -184.75555419921875 and parameters: {'n_latent': 24, 'f_n_layers': 1, 'Q_n_layers': 3, 'f_hidden_size': 50, 'Q_hidden_size': 913, 'learning_rate': 0.00038237421836540713}. Best is trial 37 with value: -195.24118041992188.\n",
      "[I 2025-06-12 20:20:33,978] Trial 44 finished with value: -22.46062469482422 and parameters: {'n_latent': 20, 'f_n_layers': 1, 'Q_n_layers': 3, 'f_hidden_size': 54, 'Q_hidden_size': 1024, 'learning_rate': 0.00010418841142700599}. Best is trial 37 with value: -195.24118041992188.\n",
      "[I 2025-06-12 20:39:38,930] Trial 45 finished with value: -128.74981689453125 and parameters: {'n_latent': 22, 'f_n_layers': 1, 'Q_n_layers': 3, 'f_hidden_size': 51, 'Q_hidden_size': 976, 'learning_rate': 0.00019701353429668107}. Best is trial 37 with value: -195.24118041992188.\n",
      "[I 2025-06-12 20:58:09,036] Trial 46 finished with value: -181.06497192382812 and parameters: {'n_latent': 24, 'f_n_layers': 1, 'Q_n_layers': 3, 'f_hidden_size': 58, 'Q_hidden_size': 892, 'learning_rate': 0.0002704748260696545}. Best is trial 37 with value: -195.24118041992188.\n",
      "[I 2025-06-12 21:10:54,677] Trial 47 finished with value: -116.9329833984375 and parameters: {'n_latent': 20, 'f_n_layers': 1, 'Q_n_layers': 1, 'f_hidden_size': 62, 'Q_hidden_size': 968, 'learning_rate': 0.0004613499857819786}. Best is trial 37 with value: -195.24118041992188.\n",
      "[I 2025-06-12 21:31:58,590] Trial 48 finished with value: -147.8160858154297 and parameters: {'n_latent': 24, 'f_n_layers': 1, 'Q_n_layers': 3, 'f_hidden_size': 55, 'Q_hidden_size': 863, 'learning_rate': 0.00016581241570998504}. Best is trial 37 with value: -195.24118041992188.\n",
      "[I 2025-06-12 22:02:54,561] Trial 49 finished with value: 260.7707214355469 and parameters: {'n_latent': 22, 'f_n_layers': 1, 'Q_n_layers': 3, 'f_hidden_size': 10, 'Q_hidden_size': 916, 'learning_rate': 0.0005710630843013464}. Best is trial 37 with value: -195.24118041992188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: -195.24118041992188\n",
      "  Params: {'n_latent': 25, 'f_n_layers': 1, 'Q_n_layers': 3, 'f_hidden_size': 51, 'Q_hidden_size': 941, 'learning_rate': 0.0002915168436070506}\n"
     ]
    }
   ],
   "source": [
    "#Initialise hyperparameters\n",
    "for (images, digits) in train_loader:\n",
    "    break\n",
    "batch_size = images.shape[0]\n",
    "z_batch_size = 1\n",
    "n_epochs = 20\n",
    "optuna_n_trials = 30\n",
    "load_study = True\n",
    "\n",
    "#Delete existing study if it exists\n",
    "study_name = \"VAE MNIST Optuna Study\"\n",
    "storage_url = \"sqlite:///VAE_MNIST_Optuna_Study.db\"\n",
    "if load_study == False:  \n",
    "    try:\n",
    "        optuna.delete_study(study_name=study_name, storage=storage_url)\n",
    "        print(f\"Deleted existing study: {study_name}\")\n",
    "    except KeyError:\n",
    "        print(f\"No existing study named '{study_name}' found. Creating a new one.\")\n",
    "\n",
    "# Run the study\n",
    "study = optuna.create_study(direction=\"minimize\",\n",
    "                            study_name=study_name,\n",
    "                            storage=storage_url,  \n",
    "                            load_if_exists=load_study)\n",
    "study.optimize(lambda trial: objective(trial, train_loader, test_loader, z_batch_size, n_epochs), n_trials=optuna_n_trials)\n",
    "\n",
    "# Best result\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(f\"  Value: {best_trial.value}\")\n",
    "print(f\"  Params: {best_trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna Trial Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Show optimization history (objective value per trial)\u001b[39;00m\n\u001b[32m      4\u001b[39m fig1 = plot_optimization_history(study)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mfig1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Show parameter importances\u001b[39;00m\n\u001b[32m      8\u001b[39m fig2 = plot_param_importances(study)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kperc\\VAE\\.venv\\Lib\\site-packages\\plotly\\basedatatypes.py:3436\u001b[39m, in \u001b[36mBaseFigure.show\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3403\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3404\u001b[39m \u001b[33;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[32m   3405\u001b[39m \u001b[33;03mspecified by the renderer argument\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3432\u001b[39m \u001b[33;03mNone\u001b[39;00m\n\u001b[32m   3433\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3434\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpio\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3436\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kperc\\VAE\\.venv\\Lib\\site-packages\\plotly\\io\\_renderers.py:425\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(fig, renderer, validate, **kwargs)\u001b[39m\n\u001b[32m    420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    421\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    422\u001b[39m     )\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat.__version__) < Version(\u001b[33m\"\u001b[39m\u001b[33m4.2.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    426\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    427\u001b[39m     )\n\u001b[32m    429\u001b[39m display_jupyter_version_warnings()\n\u001b[32m    431\u001b[39m ipython_display.display(bundle, raw=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_parallel_coordinate, plot_slice\n",
    "\n",
    "# Show optimization history (objective value per trial)\n",
    "fig1 = plot_optimization_history(study)\n",
    "fig1.show()\n",
    "\n",
    "# Show parameter importances\n",
    "fig2 = plot_param_importances(study)\n",
    "fig2.show()\n",
    "\n",
    "# Show parallel coordinate plot (hyperparameters vs. objective)\n",
    "fig3 = plot_parallel_coordinate(study)\n",
    "fig3.show()\n",
    "\n",
    "# Show slice plot (parameter values vs. objective)\n",
    "fig4 = plot_slice(study)\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 1/1875. Average P_log_likelihood: -23653006.0. Average KL_divergence: 681.8806762695312.\n",
      "Batch number: 625/1875. Average P_log_likelihood: -3510286.5. Average KL_divergence: 1879.5732421875.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: -2528259.25. Average KL_divergence: 3149.03369140625.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: -1934519.25. Average KL_divergence: 4542.708984375.\n",
      "\n",
      "Epoch number: 1/30.\n",
      "Train loss: 221743.453125, Train P_log_likelihood: -221581.21875, Train KL_divergence: 162.23829650878906.\n",
      "Test loss: 91323.1953125, Test P_log_likelihood: -91095.2578125, Test KL_divergence: 227.9402313232422.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: -1405023.625. Average KL_divergence: 3817.7001953125.\n",
      "Batch number: 625/1875. Average P_log_likelihood: -825151.0. Average KL_divergence: 16965.568359375.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: -649022.125. Average KL_divergence: 22348.12109375.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: -401993.0. Average KL_divergence: 31163.421875.\n",
      "\n",
      "Epoch number: 2/30.\n",
      "Train loss: 50371.8984375, Train P_log_likelihood: -49134.01171875, Train KL_divergence: 1237.888427734375.\n",
      "Test loss: 29602.193359375, Test P_log_likelihood: -27505.935546875, Test KL_divergence: 2096.258544921875.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: -510247.0. Average KL_divergence: 31445.1875.\n",
      "Batch number: 625/1875. Average P_log_likelihood: -394244.96875. Average KL_divergence: 41686.05859375.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: -237294.5625. Average KL_divergence: 41324.4765625.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: -207611.796875. Average KL_divergence: 41726.2109375.\n",
      "\n",
      "Epoch number: 3/30.\n",
      "Train loss: 21934.0390625, Train P_log_likelihood: -19521.169921875, Train KL_divergence: 2412.869384765625.\n",
      "Test loss: 15345.080078125, Test P_log_likelihood: -12658.9423828125, Test KL_divergence: 2686.1376953125.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: -178576.84375. Average KL_divergence: 43493.921875.\n",
      "Batch number: 625/1875. Average P_log_likelihood: -192615.28125. Average KL_divergence: 41408.00390625.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: -166104.5. Average KL_divergence: 37887.10546875.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: -135873.515625. Average KL_divergence: 37961.10546875.\n",
      "\n",
      "Epoch number: 4/30.\n",
      "Train loss: 12013.5419921875, Train P_log_likelihood: -9572.900390625, Train KL_divergence: 2440.641357421875.\n",
      "Test loss: 8624.2373046875, Test P_log_likelihood: -6302.38525390625, Test KL_divergence: 2321.851806640625.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: -92308.875. Average KL_divergence: 38795.16015625.\n",
      "Batch number: 625/1875. Average P_log_likelihood: -86547.921875. Average KL_divergence: 32120.990234375.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: -56736.78125. Average KL_divergence: 29090.92578125.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: -56824.6484375. Average KL_divergence: 28844.544921875.\n",
      "\n",
      "Epoch number: 5/30.\n",
      "Train loss: 6596.890625, Train P_log_likelihood: -4648.2900390625, Train KL_divergence: 1948.600830078125.\n",
      "Test loss: 4630.49560546875, Test P_log_likelihood: -2881.11083984375, Test KL_divergence: 1749.384765625.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: -55531.390625. Average KL_divergence: 27050.91015625.\n",
      "Batch number: 625/1875. Average P_log_likelihood: -29699.5703125. Average KL_divergence: 22433.90234375.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: -23768.685546875. Average KL_divergence: 19962.03125.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: -19006.8359375. Average KL_divergence: 17677.3984375.\n",
      "\n",
      "Epoch number: 6/30.\n",
      "Train loss: 3423.57275390625, Train P_log_likelihood: -2035.51953125, Train KL_divergence: 1388.05322265625.\n",
      "Test loss: 2347.4970703125, Test P_log_likelihood: -1147.6187744140625, Test KL_divergence: 1199.87841796875.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: -27423.810546875. Average KL_divergence: 18989.556640625.\n",
      "Batch number: 625/1875. Average P_log_likelihood: -16052.15234375. Average KL_divergence: 14892.193359375.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: -11238.76171875. Average KL_divergence: 13940.0908203125.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: -5519.43017578125. Average KL_divergence: 12101.1640625.\n",
      "\n",
      "Epoch number: 7/30.\n",
      "Train loss: 1664.261474609375, Train P_log_likelihood: -736.9345092773438, Train KL_divergence: 927.3270263671875.\n",
      "Test loss: 1078.928955078125, Test P_log_likelihood: -317.08624267578125, Test KL_divergence: 761.8427124023438.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: -2696.46240234375. Average KL_divergence: 11939.8056640625.\n",
      "Batch number: 625/1875. Average P_log_likelihood: -4443.080078125. Average KL_divergence: 10155.59765625.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 1165.115478515625. Average KL_divergence: 9198.029296875.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 515.1688842773438. Average KL_divergence: 8081.75341796875.\n",
      "\n",
      "Epoch number: 8/30.\n",
      "Train loss: 742.7406005859375, Train P_log_likelihood: -144.74200439453125, Train KL_divergence: 597.9985961914062.\n",
      "Test loss: 461.780517578125, Test P_log_likelihood: 37.796566009521484, Test KL_divergence: 499.57708740234375.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 2590.227294921875. Average KL_divergence: 7674.14794921875.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 611.6898193359375. Average KL_divergence: 6701.53173828125.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 1882.835693359375. Average KL_divergence: 5612.64599609375.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 2407.256103515625. Average KL_divergence: 5122.9384765625.\n",
      "\n",
      "Epoch number: 9/30.\n",
      "Train loss: 298.23211669921875, Train P_log_likelihood: 85.46665954589844, Train KL_divergence: 383.69879150390625.\n",
      "Test loss: 169.27615356445312, Test P_log_likelihood: 147.98190307617188, Test KL_divergence: 317.258056640625.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 1585.9176025390625. Average KL_divergence: 5079.02099609375.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 1276.11376953125. Average KL_divergence: 4393.23388671875.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 2445.72119140625. Average KL_divergence: 3752.494873046875.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 2147.72607421875. Average KL_divergence: 3393.01806640625.\n",
      "\n",
      "Epoch number: 10/30.\n",
      "Train loss: 106.26724243164062, Train P_log_likelihood: 150.8876953125, Train KL_divergence: 257.1549377441406.\n",
      "Test loss: 45.217864990234375, Test P_log_likelihood: 170.77243041992188, Test KL_divergence: 215.99029541015625.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 2377.1484375. Average KL_divergence: 3412.41796875.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 3355.09423828125. Average KL_divergence: 3254.81787109375.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 1904.13671875. Average KL_divergence: 2767.6005859375.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 2684.0888671875. Average KL_divergence: 2576.7744140625.\n",
      "\n",
      "Epoch number: 11/30.\n",
      "Train loss: 25.905136108398438, Train P_log_likelihood: 163.2808074951172, Train KL_divergence: 189.18594360351562.\n",
      "Test loss: -12.513916015625, Test P_log_likelihood: 180.5859832763672, Test KL_divergence: 168.0720672607422.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 2790.460205078125. Average KL_divergence: 2761.608154296875.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 3031.203125. Average KL_divergence: 2537.111572265625.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 2582.056640625. Average KL_divergence: 2472.1630859375.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 3031.219482421875. Average KL_divergence: 2435.808349609375.\n",
      "\n",
      "Epoch number: 12/30.\n",
      "Train loss: -21.20977783203125, Train P_log_likelihood: 175.15562438964844, Train KL_divergence: 153.9458465576172.\n",
      "Test loss: -48.58918762207031, Test P_log_likelihood: 191.11819458007812, Test KL_divergence: 142.5290069580078.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 3135.31787109375. Average KL_divergence: 2374.565185546875.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 3333.996826171875. Average KL_divergence: 2026.2564697265625.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 3620.62646484375. Average KL_divergence: 1997.185791015625.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 2015.1259765625. Average KL_divergence: 2109.653076171875.\n",
      "\n",
      "Epoch number: 13/30.\n",
      "Train loss: -56.25965881347656, Train P_log_likelihood: 189.47279357910156, Train KL_divergence: 133.213134765625.\n",
      "Test loss: -77.19524383544922, Test P_log_likelihood: 203.69566345214844, Test KL_divergence: 126.50041961669922.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 2951.2197265625. Average KL_divergence: 2041.73095703125.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 3069.227294921875. Average KL_divergence: 1979.3377685546875.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 2692.488037109375. Average KL_divergence: 1980.63525390625.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 3302.88525390625. Average KL_divergence: 1797.73681640625.\n",
      "\n",
      "Epoch number: 14/30.\n",
      "Train loss: -83.4450454711914, Train P_log_likelihood: 202.9134063720703, Train KL_divergence: 119.4683609008789.\n",
      "Test loss: -105.40022277832031, Test P_log_likelihood: 215.9552459716797, Test KL_divergence: 110.55502319335938.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 2654.322998046875. Average KL_divergence: 1690.71728515625.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 3632.499267578125. Average KL_divergence: 1926.177490234375.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 3878.386474609375. Average KL_divergence: 1828.95654296875.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 3175.5380859375. Average KL_divergence: 1636.5811767578125.\n",
      "\n",
      "Epoch number: 15/30.\n",
      "Train loss: -106.96919250488281, Train P_log_likelihood: 216.52865600585938, Train KL_divergence: 109.55946350097656.\n",
      "Test loss: -123.4710693359375, Test P_log_likelihood: 225.904052734375, Test KL_divergence: 102.4329833984375.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 3001.958251953125. Average KL_divergence: 1682.359619140625.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 4592.986328125. Average KL_divergence: 1677.54638671875.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 4260.9482421875. Average KL_divergence: 1621.4971923828125.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 3798.995361328125. Average KL_divergence: 1622.4765625.\n",
      "\n",
      "Epoch number: 16/30.\n",
      "Train loss: -124.78585815429688, Train P_log_likelihood: 226.75942993164062, Train KL_divergence: 101.97357177734375.\n",
      "Test loss: -135.37606811523438, Test P_log_likelihood: 232.81857299804688, Test KL_divergence: 97.44249725341797.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 2588.72998046875. Average KL_divergence: 1630.4810791015625.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 4623.3525390625. Average KL_divergence: 1494.458251953125.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 3553.47216796875. Average KL_divergence: 1555.380126953125.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 4676.09326171875. Average KL_divergence: 1601.1737060546875.\n",
      "\n",
      "Epoch number: 17/30.\n",
      "Train loss: -141.4983673095703, Train P_log_likelihood: 237.45883178710938, Train KL_divergence: 95.96046447753906.\n",
      "Test loss: -153.92047119140625, Test P_log_likelihood: 248.44281005859375, Test KL_divergence: 94.52234649658203.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 3393.406005859375. Average KL_divergence: 1566.843017578125.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 3702.796630859375. Average KL_divergence: 1472.19921875.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 2909.403564453125. Average KL_divergence: 1426.5328369140625.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 4668.705078125. Average KL_divergence: 1391.206298828125.\n",
      "\n",
      "Epoch number: 18/30.\n",
      "Train loss: -154.97366333007812, Train P_log_likelihood: 245.94224548339844, Train KL_divergence: 90.96858978271484.\n",
      "Test loss: -162.046875, Test P_log_likelihood: 249.857421875, Test KL_divergence: 87.81053924560547.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 3221.122314453125. Average KL_divergence: 1491.089111328125.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 2528.392822265625. Average KL_divergence: 1500.42529296875.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 3736.2021484375. Average KL_divergence: 1284.4638671875.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 4154.7392578125. Average KL_divergence: 1431.7879638671875.\n",
      "\n",
      "Epoch number: 19/30.\n",
      "Train loss: -166.46380615234375, Train P_log_likelihood: 253.1499786376953, Train KL_divergence: 86.6861801147461.\n",
      "Test loss: -173.62387084960938, Test P_log_likelihood: 255.64425659179688, Test KL_divergence: 82.02037811279297.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 4736.47265625. Average KL_divergence: 1341.994140625.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 3121.07080078125. Average KL_divergence: 1434.6729736328125.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 4483.70703125. Average KL_divergence: 1440.478271484375.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 4290.54736328125. Average KL_divergence: 1351.082763671875.\n",
      "\n",
      "Epoch number: 20/30.\n",
      "Train loss: -176.3915557861328, Train P_log_likelihood: 259.3538818359375, Train KL_divergence: 82.96232604980469.\n",
      "Test loss: -183.21063232421875, Test P_log_likelihood: 264.9801330566406, Test KL_divergence: 81.7695083618164.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 4760.65673828125. Average KL_divergence: 1269.8433837890625.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 4259.3681640625. Average KL_divergence: 1400.3153076171875.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 4126.36572265625. Average KL_divergence: 1226.6683349609375.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 4441.97021484375. Average KL_divergence: 1184.614990234375.\n",
      "\n",
      "Epoch number: 21/30.\n",
      "Train loss: -185.3555450439453, Train P_log_likelihood: 265.1180419921875, Train KL_divergence: 79.76249694824219.\n",
      "Test loss: -190.1144256591797, Test P_log_likelihood: 267.0774841308594, Test KL_divergence: 76.96305847167969.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 4099.43505859375. Average KL_divergence: 1285.061279296875.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 5284.97802734375. Average KL_divergence: 1185.9014892578125.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 4172.6650390625. Average KL_divergence: 1176.94775390625.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 4344.87353515625. Average KL_divergence: 1242.644775390625.\n",
      "\n",
      "Epoch number: 22/30.\n",
      "Train loss: -191.86068725585938, Train P_log_likelihood: 268.72796630859375, Train KL_divergence: 76.86727905273438.\n",
      "Test loss: -192.3482208251953, Test P_log_likelihood: 264.7785339355469, Test KL_divergence: 72.43031311035156.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 3387.24560546875. Average KL_divergence: 1164.947021484375.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 4923.0693359375. Average KL_divergence: 1226.0245361328125.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 4285.751953125. Average KL_divergence: 1112.756591796875.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 4317.70068359375. Average KL_divergence: 1180.4658203125.\n",
      "\n",
      "Epoch number: 23/30.\n",
      "Train loss: -198.93150329589844, Train P_log_likelihood: 273.2715148925781, Train KL_divergence: 74.34001159667969.\n",
      "Test loss: -200.30755615234375, Test P_log_likelihood: 270.950927734375, Test KL_divergence: 70.64337921142578.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 4196.93896484375. Average KL_divergence: 1167.3681640625.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 5189.63427734375. Average KL_divergence: 1142.70556640625.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 3731.72119140625. Average KL_divergence: 1136.42919921875.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 4475.2197265625. Average KL_divergence: 1107.269287109375.\n",
      "\n",
      "Epoch number: 24/30.\n",
      "Train loss: -204.35812377929688, Train P_log_likelihood: 276.4306945800781, Train KL_divergence: 72.07257843017578.\n",
      "Test loss: -207.88372802734375, Test P_log_likelihood: 278.093017578125, Test KL_divergence: 70.20928192138672.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 5359.61767578125. Average KL_divergence: 1140.212646484375.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 4603.427734375. Average KL_divergence: 1098.521484375.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 3384.1435546875. Average KL_divergence: 1094.9034423828125.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 3682.591796875. Average KL_divergence: 1119.35107421875.\n",
      "\n",
      "Epoch number: 25/30.\n",
      "Train loss: -209.96881103515625, Train P_log_likelihood: 280.05828857421875, Train KL_divergence: 70.08948516845703.\n",
      "Test loss: -206.32162475585938, Test P_log_likelihood: 274.222412109375, Test KL_divergence: 67.9007797241211.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 4852.4033203125. Average KL_divergence: 1089.9718017578125.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 4919.2431640625. Average KL_divergence: 1069.3720703125.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 4231.5615234375. Average KL_divergence: 1102.9632568359375.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 5623.068359375. Average KL_divergence: 1030.133056640625.\n",
      "\n",
      "Epoch number: 26/30.\n",
      "Train loss: -214.19891357421875, Train P_log_likelihood: 282.3841857910156, Train KL_divergence: 68.18527221679688.\n",
      "Test loss: -213.97421264648438, Test P_log_likelihood: 279.7515869140625, Test KL_divergence: 65.7773666381836.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 4384.072265625. Average KL_divergence: 1077.2767333984375.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 4287.4169921875. Average KL_divergence: 1103.1422119140625.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 4765.61181640625. Average KL_divergence: 971.3406982421875.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 4489.3037109375. Average KL_divergence: 1018.2025146484375.\n",
      "\n",
      "Epoch number: 27/30.\n",
      "Train loss: -218.5972900390625, Train P_log_likelihood: 285.0860900878906, Train KL_divergence: 66.48880767822266.\n",
      "Test loss: -213.22488403320312, Test P_log_likelihood: 277.9356384277344, Test KL_divergence: 64.71076202392578.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 4053.681884765625. Average KL_divergence: 1103.4456787109375.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 5465.60205078125. Average KL_divergence: 1005.51318359375.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 4439.7333984375. Average KL_divergence: 1027.1898193359375.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 4128.17626953125. Average KL_divergence: 1056.296875.\n",
      "\n",
      "Epoch number: 28/30.\n",
      "Train loss: -222.36001586914062, Train P_log_likelihood: 287.3004150390625, Train KL_divergence: 64.9404067993164.\n",
      "Test loss: -209.65567016601562, Test P_log_likelihood: 273.75274658203125, Test KL_divergence: 64.09708404541016.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 5343.306640625. Average KL_divergence: 998.658935546875.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 4323.7578125. Average KL_divergence: 1004.2670288085938.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 5708.48291015625. Average KL_divergence: 951.7108764648438.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 4808.79052734375. Average KL_divergence: 1022.7481689453125.\n",
      "\n",
      "Epoch number: 29/30.\n",
      "Train loss: -226.1233367919922, Train P_log_likelihood: 289.6996154785156, Train KL_divergence: 63.57627868652344.\n",
      "Test loss: -222.61363220214844, Test P_log_likelihood: 285.11083984375, Test KL_divergence: 62.49721145629883.\n",
      "\n",
      "Batch number: 1/1875. Average P_log_likelihood: 6278.80859375. Average KL_divergence: 984.1369018554688.\n",
      "Batch number: 625/1875. Average P_log_likelihood: 4068.734375. Average KL_divergence: 1016.8758544921875.\n",
      "Batch number: 1250/1875. Average P_log_likelihood: 4972.54833984375. Average KL_divergence: 1001.2427368164062.\n",
      "Batch number: 1875/1875. Average P_log_likelihood: 4179.34521484375. Average KL_divergence: 1065.35791015625.\n",
      "\n",
      "Epoch number: 30/30.\n",
      "Train loss: -229.110107421875, Train P_log_likelihood: 291.3507080078125, Train KL_divergence: 62.24059295654297.\n",
      "Test loss: -223.28842163085938, Test P_log_likelihood: 286.04266357421875, Test KL_divergence: 62.754241943359375.\n",
      "\n",
      "\n",
      "Re-training Complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the best trial's parameters\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "#Increase number of epochs for final training\n",
    "n_epochs += 10\n",
    "\n",
    "# Rebuild the model using the best parameters\n",
    "n_latent = best_params[\"n_latent\"]\n",
    "f_n_layers = best_params[\"f_n_layers\"]\n",
    "Q_n_layers = best_params[\"Q_n_layers\"]\n",
    "f_hidden_size = best_params[\"f_hidden_size\"]\n",
    "Q_hidden_size = best_params[\"Q_hidden_size\"]\n",
    "learning_rate = best_params[\"learning_rate\"]\n",
    "\n",
    "f_hidden_sizes = [f_hidden_size for _ in range(f_n_layers)]\n",
    "Q_hidden_sizes = [Q_hidden_size for _ in range(Q_n_layers)]\n",
    "\n",
    "# Get n_observed from your data\n",
    "for (images, digits) in train_loader:\n",
    "    break\n",
    "n_observed = torch.prod(torch.tensor(images[(0,) + (slice(None),) * (images.dim() - 1)].shape))\n",
    "\n",
    "# Rebuild and retrain the model\n",
    "best_model = VAE(n_observed, n_latent, f_hidden_sizes, Q_hidden_sizes, z_batch_size)\n",
    "optimiser = torch.optim.Adam(best_model.parameters(), lr=learning_rate)\n",
    "best_model, epoch_train_losses, epoch_test_losses = train_VAE(train_loader, test_loader, best_model, n_epochs, optimiser, print_flag = True)\n",
    "\n",
    "print(\"\\nRe-training Complete.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training and testing losses over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot train and test losses\n",
    "plt.plot(epoch_train_losses.detach().numpy(), label='Train Loss')\n",
    "plt.plot(epoch_test_losses.detach().numpy(), label='Test Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train vs Test Loss Throughout Training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample a batch from the test dataloader, then sample from the model to get similar data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Create new images via sampling.\n",
      "\n",
      "\n",
      "(32, 1, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Create similar images to test dataset.\n",
      "\n",
      "\n",
      "\n",
      "True image.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similar image.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create new batch by sampling from standard normal distribution\n",
    "#Shape we need: (batch_size, z_batch_size, n_latent)\n",
    "sampled_z = torch.randn((batch_size, z_batch_size, n_latent))\n",
    "\n",
    "#Run model with sampled z\n",
    "P = best_model(sampled_z, skip_encode=True)\n",
    "\n",
    "#Extract the mean of the distribution P. This is a point estimate for the reconstructed data.\n",
    "#We also take the mean over z_batch_size, as we want a single point estimate for each data point.\n",
    "P_mu = P.loc\n",
    "P_mu = torch.mean(P_mu, dim=1)\n",
    "\n",
    "#Visualise the reconstructed images\n",
    "print(\"\\n\\nCreate new images via sampling.\\n\\n\")\n",
    "images = image_reshape(P_mu).detach().numpy()\n",
    "print(images.shape)\n",
    "plt.imshow(images[5, 0, :, :], cmap='gray')\n",
    "plt.title(\"28x28 Tensor Image\")\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#print(\"first image: \", images[0, 0, :, :])\n",
    "\n",
    "#Alternatively, we can try to sample points similar to a given image.\n",
    "print(\"\\n\\nCreate similar images to test dataset.\\n\\n\")\n",
    "for (images, digits) in train_loader:\n",
    "    z, Q, P = best_model(images, skip_encode = False)\n",
    "\n",
    "    similar_data = P.sample((1,)) \n",
    "    similar_data = torch.clamp(similar_data, -1, 1)\n",
    "\n",
    "    #Reshape images\n",
    "    images = image_reshape(images)\n",
    "    similar_images = image_reshape(similar_data).detach().numpy()\n",
    "\n",
    "    #Plot the true first image, then the reconstructed image\n",
    "    print(\"\\nTrue image.\\n\")\n",
    "    plt.imshow(images[0, 0, :, :], cmap='gray')\n",
    "    plt.title(\"28x28 Tensor Image\")\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nSimilar image.\\n\")\n",
    "    plt.imshow(similar_images[0, 0, :, :], cmap='gray')\n",
    "    plt.title(\"28x28 Tensor Image\")\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "#TODO:\n",
    "#1. Scaling at end of model - must be between 0 and 1.\n",
    "#^But, we get a distribution, so how do we scale? <-- Jut apply softmax to the mean, then will\n",
    "#eventually clamp values to ensure between 0 and 1.\n",
    "\n",
    "#2. How to aggregate over z_sample_size? Do we have to have the same number for z_sample\n",
    "#3. How to use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "Will compare to working code from https://github.com/williamcfrancis/Variational-Autoencoder-for-MNIST\n",
    "\n",
    "Things to compare:\n",
    "- loss calculation\n",
    "- hidden sizes and other hyperparameters?\n",
    "- \n",
    "\n",
    "^Try to change other working VAE to use Gaussians instead of Bernoulli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfjEK2ePMHON"
   },
   "source": [
    "## Notes\n",
    "\n",
    "- Will use $P(X|z) = N(X|f(z; \\theta), \\sigma^2 * I)$ for a loss function. We wish to maximise this, or equivalently to minimise $-P(X|z)$.\n",
    "- Could simplify by only feeding through distributions to loss function and leaving it to Pytorch to compute KL-divergence. This would be very simple and allows generalisation to non-normal distributions.\n",
    "- We will start by minimising $\\sigma^2$ as a global parameter, like in the textbook \"Understanding \n",
    "Deep Learning\". But, could improve this in the future.\n",
    "- Currently estimating $\\mu(X; \\phi)$ and $\\Sigma(X; \\phi)$ through a single neural network, although\n",
    "could separate this...\n",
    "- We apply a sigmoid function to ensure that the mean of $P(X|z)$, $f(z; \\theta)$ is between 0 and 1. But, if we sample from the normal $P(X|z)$, we need to clamp the elements so that they fall between 0 and 1 as well (as normal distributions can take any real value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO - basic\n",
    "- Check: we have to sample z from Q before we can actually train the neural network f, right?\n",
    "- Consider how to optimise sigma globally in practice.\n",
    "- Must add function to ensure that output of VAE is in the correct form, i.e. a picture of integers between 0 and 255?\n",
    "- Add dropout or some other bells and whistles?\n",
    "- KL divergence is collapsing to zero, so will try to use a smaller decoder. Recall that Q is the encoder and P is the encoder, so we will reduce the suggested f_hidden_size\n",
    "\n",
    "### TODO - future\n",
    "- Add preprocessing - is it best practice to standardise variables to have mean 0 and variance 1?\n",
    "- Add conditional VAE theory\n",
    "- Add recent research in Beta-Sigma VAE: https://arxiv.org/pdf/2409.09361"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
