{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cR2wBH7bGZhK"
   },
   "source": [
    "# VAE Implementation\n",
    "\n",
    "All theory is from https://arxiv.org/abs/1606.05908.\n",
    "\n",
    "Notation:\n",
    "- $X$ is a data point in our dataset\n",
    "- $z$ is the latent representation of a data point\n",
    "- $\\theta$ is the parameters for our model, which will be a distribution for $P(X|z;\\theta)$. Specifically, our model is a mapping from the latent representation to a data point.\n",
    "- $D$ is our entire dataset, so $X\\in D$.\n",
    "<!-- \n",
    "- $f(z;\\theta)$ is a map from latent variables z to a point estimate for a data point, X.)\n",
    "<> - $Q(z|X)$ is a \"surrogate\" distribution to approximate the probability $P(z|X)$. Assuming we choose that Q(z|X) is normal, then $\\mu(X;\\phi)$ and $\\Sigma(X;\\phi)$ are the mean and variance, respectively.\n",
    "-->\n",
    "\n",
    "\n",
    "Explanation\n",
    "- We wish to maximising the probability of our data under our model,\n",
    "\\begin{equation}\n",
    "P(X) = \\int P(X|z; \\theta) P(z)dz.\n",
    "\\end{equation}\n",
    "- When $X$ is continuous, it is common to use a normal distribution for our model:\n",
    "$$P(X|z\\theta) = N(f(z;\\theta), \\sigma^2 I).$$\n",
    "Here, $f(z;\\theta)$ is our point estimate for a mapping from the latent representation to the data points, i.e. $f:\\mathcal{Z} \\times \\Theta\\rightarrow \\mathcal{X}$, where any $z\\in \\mathcal{Z}$, $\\theta \\in \\Theta$ and $X \\in \\mathcal{X}$. Also, $\\sigma^2$ is a hyperparameter of the model, and is set beforehand.\n",
    "- We say that the samples of $z$ are from a $N(0, I)$ distribution, so $f(z; \\theta)$ is left with the responsiblity of first converting these normal samples into latent variables which are more representative/useful. This uses the fact that any distribution in $d$ dimensions can be generated by taking a set of $d$ variables that are normally distributed and mapping them through a sufficiently complicated function (this is an extension of inverse transform sampling).\n",
    "- To allow sufficient flexibility, $f(z;\\theta)$ is usually a multi-layer neural network. This allows us to learn a map from $z\\sim N(0, \\sigma^2 I)$ to something which \"looks like\" a data point, $X$. \n",
    "- A naive approach would be to sample $z_1, ..., z_n$ for very large $n$, then approximate our model evidence as $P(X) \\approx \\frac{1}{n} \\sum_{i=1}^n P(X|z_i; \\theta)$. From here we could use gradient descent to optimise $f(z; \\theta)$ by maximising $P(X)$. The problem is that in higher dimensional space, $n$ might have to be extremely large before we get an accurate estimate. This is because usually, for most $z$, $P(X|z)$ will be nearly zero. The key idea of Variational Autoencoders is to attempt to sample values of $z$ that are likely to have produced $X$, and then compute $P(X)$ just from these...\n",
    "- The definition of KL-divergence is\n",
    "\\begin{equation}\n",
    "D_{KL}(P||Q) = \\int P(x) \\log \\left(\\frac{P(x)}{Q(x)}\\right)dx.\n",
    "\\end{equation}\n",
    "Equivalently,\n",
    "\\begin{equation}\n",
    "D_{KL}(P||Q) = \\mathbb{E}_{X\\sim P} \\left[ \\log\\left(\\frac{P(x)}{Q(x)}\\right)\\right] = \\mathbb{E}_{X\\sim P} \\left[ \\log P(x) - \\log Q(x) \\right].\n",
    "\\end{equation}\n",
    "- In VAEs, we explained earlier that we want to sample the values of $z$ which are likely to have produced any particular data point, $X$. We use a \"surrogate\" distribution to approximate $P(z|X)$, $Q(z|X)$. Then, the KL-divergence between our surrogate distribution $Q(z|X)$ and the probability of latent variables in a particular data point, $P(z|X)$ is given by\n",
    "\\begin{equation*}\n",
    "D_{KL}(Q(z|X) || P(z|X)) = \\mathbb{E}_{z\\sim Q} \\left[ \\log Q(z|X) - \\log P(z|X) \\right].\n",
    "\\end{equation*}\n",
    "Now we can apply Bayes Rule:\n",
    "\\begin{align}\n",
    "P(z|X) &= \\frac{P(X|z) P(z)}{P(X)} \\implies D_{KL}(Q(z|X) || P(z|X))\\\\ \n",
    "&= \\mathbb{E}_{z\\sim Q} \\left[ \\log Q(z|X) - \\log \\left( \\frac{P(X|z) P(z)}{P(X)}\\right)\\right]\\\\\n",
    "&= \\mathbb{E}_{z\\sim Q} \\left[ \\log \\left( \\frac{Q(z|X)}{P(z)}\\right) - \\log P(X|z) \\right] + \\log P(X)\\\\\n",
    "&= D_{KL}(Q(z|X) || P(z)) - \\mathbb{E}_{z\\sim Q}\\left[ \\log P(X|z) \\right] + \\log P(X).\\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\therefore \\log P(X) - F_{KL}(Q(z|X) || P(z)) = \\mathbb{E}_{z\\sim Q}\\left[ \\log P(X|z)\\right] - D_{KL}(Q(z|X) || P(z))\n",
    "\\end{equation*}\n",
    "\n",
    "We cannot compute $D_{KL}(Q(z|X) || P(z|X))$ without knowing $P(z|X)$, but a property of KL-divergence is that it is non-negative. Therefore, our expression becomes\n",
    "\\begin{equation*}\n",
    "\\log P(X) \\geq \\mathbb{E}_{z\\sim Q}\\left[ \\log P(X|z)\\right] - D_{KL}(Q(z|X) || P(z)).\n",
    "\\end{equation*}\n",
    "The LHS is called the evidence, hence the RHS is called the \"Evidence Lower BOund\" (ELBO). We wish to maximies the evidence (as this corresponds to the observed data being likely under our model), so in VAEs we do this by maximising the RHS and treating $D_{KL}(Q(z|X) || P(z|X))$ as an error term.\n",
    "- Typically, we set our surrogate distribution as $Q(z|X) \\sim N(\\mu(X; \\phi), \\Sigma(X; \\phi))$. In practice, $\\mu(X; \\phi)$ and $\\Sigma(X;\\phi)$ are again obtained using neural networks, and these are maps from a data point $X$ to the estimated mean and variance for the latent variables, respectively. Hence, the parameters $\\phi$ are learned from the data. Note that $\\Sigma$ is constrained to be a diagonal matrix, as independence between components is necessary for computational tractibility and encourages disentanglement of latent variables.\n",
    "- In the ELBO, the term $D_{KL}(Q(z|X) || P(z))$ is a KL-divergence between multivariate Gaussian distributions, hence a closed form expression for the KL-divergence is known. In our case, \n",
    "\\begin{align}\n",
    "D_{KL}(Q(z|X) || P(z)) &= D_{KL}(N(\\mu(X), \\Sigma(X)) || N(0, I))\\\\\n",
    "&= \\frac{1}{2}\\left( \\text{trace}(\\Sigma(X)) + (\\mu(X))^T (\\mu(X)) - k - \\log \\text{det}(\\Sigma(X))\\right)\n",
    "\\end{align}\n",
    "\n",
    "But, in the ELBO it is more difficult to compute the term $\\mathbb{E}_{z \\sim Q} \\left[ log P(X|z) \\right]$. One approach is to estimate this expectation by sampling many $z$, although this will be expensive. Instead, we can apply the idea from stochastic gradient descent and use a single sample of $z$ to approximate this expectation for a single data point, X. Then, our total loss (over the entire dataset $D$) to minimise with stochastic gradient descent is\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{X\\sim D} \\left[ \\mathbb{E}_{z \\sim Q} \\left[ \\log P(X|z) \\right] - D_{KL} (Q(z|X) || P(z)) \\right].\n",
    "\\end{equation}\n",
    "We estimate the gradient of this by averaging the gradient of\n",
    "\\begin{equation}\n",
    "\\log P(X|z) - D_{KL}(Q(z|X) || P(z))\n",
    "\\end{equation}\n",
    "for an arbitrary number of samples of $z$ and $X$. The result will converge to the overall gradient (for all $X$, $z$).\n",
    "\n",
    "- But, the issue with the above gradient estimate is that we can't actually backpropogate through $\\mathbb{E}_{z\\sim Q} \\left[ \\log P(X|z) \\right]$, as the expectation $\\mathbb{E}_{z \\sim Q} \\left[ \\cdot \\right]$ depends on $\\phi$. Specifically, in general $\\triangledown_{\\phi} \\mathbb{E}_{z\\sim Q} \\left[ \\log P(X|z) \\right] \\neq \\mathbb{E}_{z\\sim Q} \\left[ \\triangledown_{\\phi} \\log P(X|z) \\right]$. In integral form,\n",
    "\\begin{align}\n",
    "\\triangledown_{\\phi} \\mathbb{E}_{z\\sim Q} \\left[ \\log P(X|z) \\right] &= \\int_z \\triangledown_{\\phi} ( Q(z|X) \\log P(X|z) ) dz\\\\\n",
    "&= \\int_z \\triangledown_{\\phi} (Q(z|X)) \\log P(X|z)dz + \\int_z Q(z|X) \\triangledown_{\\phi} ( \\log P(X|z) ) dz\\\\\n",
    "&= \\int_z \\triangledown_{\\phi} (Q(z|X)) \\log P(X|z)dz + \\mathbb{E}_{z \\sim Q} \\left[ \\triangledown_{\\phi} (\\log P(X|z) ) \\right].\n",
    "\\end{align}\n",
    "\n",
    "We get around this issue using the reparametrisation trick. We instead convert the random sampling into a (stochastic) input, and then write $Q(z|X)$ as a deterministic function of these stochastic inputs. For example, when $Q(z|X)\\sim N(\\mu(X; \\phi), \\Sigma(X; \\phi))$ can be written as \n",
    "$$Q(z|X) = \\mu(X; \\phi) + (\\Sigma(X; \\phi))^{\\frac{1}{2}} \\cdot \\epsilon,$$ \n",
    "where $\\epsilon \\sim N(0, I)$. In general, we want to sample from $Q(z|X)$ by evaluating a function $h(\\nu, X)$, where $\\nu$ is noise from a distribution that is not learned (i.e. has fixed inputs that don't depend on $\\phi$). Also, $h$ must be continuous in $X$ so that we can backpropogate through it. \n",
    "\n",
    "Things to estimate in VAE:\n",
    "- $f(z; \\theta)$, neural network. Maps $z$ to $X$.\n",
    "- $\\mu(X; \\phi)$ and $\\Sigma(X; \\phi)$, neural network. Parameters of the surrogate $Q(z|X)$ for $P(z|X)$. $\\mu$ maps the data point $X$ to a point estimate of the mean for the latent variables $z$, and $\\Sigma$ maps $X$ to the estimated variance for $z$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first apply to the MNIST Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GcKNKGJFGSO1"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransforms\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1] range\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 64  # You can change this\n",
    "\n",
    "# Load training dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, transform=transform, download=True\n",
    ")\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "id": "0D0525-xHtHg",
    "outputId": "f79d0d3d-3ffb-4b04-dde0-13a63954c12e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get a batch of images\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Plot first 5 images\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(images[i].squeeze(), cmap=\"gray\")\n",
    "    plt.title(f\"Label: {labels[i].item()}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyyKatYpIsCD"
   },
   "source": [
    "## VAE Implementation\n",
    "\n",
    "Things to do:\n",
    "- train neural network for $f(z; \\theta)$. But, do we want to use the error term to select hyperparameters??\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1ZcpDhMIrSt"
   },
   "outputs": [],
   "source": [
    "#Functions necessary to train and validate VAE\n",
    "\n",
    "def train_VAE(train_dataloader):\n",
    "  \"\"\"\n",
    "  Function to train the VAE.\n",
    "\n",
    "  Inputs:\n",
    "  - train_dataloader: the `DataLoader` object for the training dataset.\n",
    "  \"\"\"\n",
    "\n",
    "  return None\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_VAE(test_dataloader):\n",
    "  \"\"\"\n",
    "  Function to evaluate the VAE.\n",
    "\n",
    "  Inputs:\n",
    "  - test_dataloader: the `DataLoader` object for the test dataset.\n",
    "  \"\"\"\n",
    "\n",
    "  return None\n",
    "\n",
    "\n",
    "\n",
    "def train_f(z, sigma):\n",
    "  \"\"\"\n",
    "  Function to learn the function f(z; theta), which maps latent representations\n",
    "  to data points.\n",
    "\n",
    "  Inputs:\n",
    "  - z: the input latent representation.\n",
    "  - sigma: this is the\n",
    "\n",
    "  Outputs:\n",
    "  - f: the learned function from latent representations to data points.\n",
    "  \"\"\"\n",
    "\n",
    "  f = None\n",
    "\n",
    "  return f\n",
    "\n",
    "\n",
    "\n",
    "class prob_X_given_z_loss(nn.module):\n",
    "  \"\"\"\n",
    "  Custom loss function to train f(z; theta)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    super(prob_X_given_z_loss, self).__init__()\n",
    "\n",
    "  def forward(self, X, f_z, sigma):\n",
    "    \"\"\"\n",
    "    Function to calculate negative likelihood, -P(X|z) = N(X|f(z; theta), sigma^2*I).\n",
    "\n",
    "    Inputs:\n",
    "    - X: the batch of data points to be evaluated.\n",
    "    - f_z: the function f(z; theta), evaluated at the\n",
    "\n",
    "\n",
    "\n",
    "    HOW DO WE KNOW THE z CORRESPONDING TO SOME X?\n",
    "\n",
    "    ^Should end up with MSE - revise deep learning textbook example.\n",
    "\n",
    "    ^Issue is that we have an extra layer of randomness compared to textbook example,\n",
    "    as we don't know the z corresponding to each data point and therefore cannot\n",
    "    compute f(z; theta) for the z corresponding to a particular X.\n",
    "\n",
    "    Instead, can we integrate over all possible z? Although this will be very\n",
    "    expensive for a loss function.\n",
    "\n",
    "    Or, the ide acould just be to sample one z and then use this? No, this\n",
    "    won't encourage continuity...\n",
    "\n",
    "\n",
    "    ^NEED TO TRAIN FOR THETA AND PHI AT THE SAME TIME! CHANGE IPAD NOTES!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Define multivariate normal distribution\n",
    "    mvn = torch.distributions.MultivariateNormal(f_z, covariance_matrix=sigma^2 * torch.eye(len(f_z)))\n",
    "\n",
    "    # Evaluate PDF at specific points\n",
    "    x = torch.tensor([[0.0, 0.0], [1.0, 1.0], [-1.0, -1.0]])  # 3 points in 2D space\n",
    "    pdf_values = mvn.log_prob(x).exp()  # Compute PDF\n",
    "\n",
    "    normal_dist = torch.distributions.Normal(f_z, sigma)\n",
    "\n",
    "    # Compute density for a given x value\n",
    "    x = torch.tensor([0.0, 1.0, -1.0, 2.0])  # Points to evaluate\n",
    "    pdf_values = normal_dist.log_prob(x).exp()  # Compute PDF\n",
    "\n",
    "    likelihood =\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfjEK2ePMHON"
   },
   "source": [
    "## Notes\n",
    "\n",
    "- Will use $P(X|z) = N(X|f(z; \\theta), \\sigma^2 * I)$ for a loss function. We wish to maximise this, or equivalently to minimise $-P(X|z)$."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
